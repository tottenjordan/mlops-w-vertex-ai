{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a39bada-044c-4b0a-b6f8-d0cc074dd183",
   "metadata": {},
   "source": [
    "# Create event-based trigger with Cloud Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e04eb-3909-4a25-98d1-caecd6a74dc7",
   "metadata": {},
   "source": [
    "Create a Cloud Function that runs the pipeline whenever new data is inserted into a BigQuery table\n",
    "\n",
    "> Specifically, we use an Eventarc to trigger the function whenever a google.`cloud.bigquery.v2.JobService.InsertJob` event occurs\n",
    "\n",
    "> refer to BQ resource as: `projects/hybrid-vertex/datasets/mlops/tables/chicago`\n",
    "\n",
    "For more information, see [Eventarc triggers](https://cloud.google.com/functions/docs/calling/eventarc) and [supported event types](https://cloud.google.com/eventarc/docs/reference/supported-events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ce26c-e084-42d6-ab97-7a3c2a7e143f",
   "metadata": {},
   "source": [
    "# Create Cloud Function trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac5b3a-54db-4992-a137-0229d63f9159",
   "metadata": {},
   "source": [
    "## Follow these steps in the console:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000fe09-ee84-4cc0-ae14-4138c0b86f57",
   "metadata": {},
   "source": [
    "### [1] In the Google Cloud console, go to the [Cloud Run functions](https://console.cloud.google.com/functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c7a6a0-d41d-403e-bc65-dbd5b8e18d2a",
   "metadata": {},
   "source": [
    "### [2] Click the Create Function button. In the Configuration page:\n",
    "\n",
    "  * Select **2nd gen** as your environment.\n",
    "  * For Function name, use **mlops**.\n",
    "  * For **Region**, select the same region as your Cloud Storage bucket and Artifact Registry repository.\n",
    "  * For **Trigger** Select **Other trigger**. The Eventarc Trigger pane opens.\n",
    "    * For **Trigger Type**, choose **Google Sources**.\n",
    "    * For **Event Provider**, choose **BigQuery**.\n",
    "    * For **Event type**, choose `google.cloud.bigquery.v2.JobService.InsertJob`\n",
    "    * For **Resource**, choose Specific resource and specify the **BigQuery table:**\n",
    "    \n",
    "        > `projects/hybrid-vertex/datasets/mlops/tables/chicago`\n",
    "    \n",
    "    * In the Region field, select a location for the Eventarc trigger, if applicable. See [Trigger location](https://cloud.google.com/functions/docs/calling/eventarc#trigger-location) for more information.\n",
    "    \n",
    "    * Click **Save Trigger**\n",
    "    \n",
    "If you are asked to grant roles to service account(s), click **Grant All**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6efe3e-b019-489c-9766-dbd8c6d8d74d",
   "metadata": {},
   "source": [
    "### [3] Click **Next** to go to the **Code page**. In the **Code page**:\n",
    "\n",
    "* Set the **Runtime** to **python 3.12**\n",
    "* Set the **Entry point** to `mlops_entrypoint`\n",
    "* Open the file `requirements.txt` and replace the contents with the following:\n",
    "\n",
    "> ```bash\n",
    "requests==2.31.0\n",
    "google-auth==2.25.1\n",
    "\n",
    "* With the **Inline Editor**, open the file `main.py` and replace the contents with the script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada2c10-48e2-4333-addc-eaa21e4a8a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "## update vars: `PROJECT_ID`,`REGION`,`BUCKET_NAME`\n",
    "## update script:\n",
    "\n",
    "# import json\n",
    "# import functions_framework\n",
    "# import requests\n",
    "# import google.auth\n",
    "# import google.auth.transport.requests\n",
    "# # CloudEvent function to be triggered by an Eventarc Cloud Audit Logging trigger\n",
    "# # Note: this is NOT designed for second-party (Cloud Audit Logs -> Pub/Sub) triggers!\n",
    "# @functions_framework.cloud_event\n",
    "# def mlops_entrypoint(cloudevent):\n",
    "#     # Print out the CloudEvent's (required) `type` property\n",
    "#     # See https://github.com/cloudevents/spec/blob/v1.0.1/spec.md#type\n",
    "#     print(f\"Event type: {cloudevent['type']}\")\n",
    "\n",
    "#     # Print out the CloudEvent's (optional) `subject` property\n",
    "#     # See https://github.com/cloudevents/spec/blob/v1.0.1/spec.md#subject\n",
    "#     if 'subject' in cloudevent:\n",
    "#         # CloudEvent objects don't support `get` operations.\n",
    "#         # Use the `in` operator to verify `subject` is present.\n",
    "#         print(f\"Subject: {cloudevent['subject']}\")\n",
    "\n",
    "#     # Print out details from the `protoPayload`\n",
    "#     # This field encapsulates a Cloud Audit Logging entry\n",
    "#     # See https://cloud.google.com/logging/docs/audit#audit_log_entry_structure\n",
    "\n",
    "#     payload = cloudevent.data.get(\"protoPayload\")\n",
    "#     if payload:\n",
    "#         print(f\"API method: {payload.get('methodName')}\")\n",
    "#         print(f\"Resource name: {payload.get('resourceName')}\")\n",
    "#         print(f\"Principal: {payload.get('authenticationInfo', dict()).get('principalEmail')}\")\n",
    "#         row_count = payload.get('metadata', dict()).get('tableDataChange',dict()).get('insertedRowsCount')\n",
    "#         print(f\"No. of rows: {row_count} !!\")\n",
    "#         if row_count:\n",
    "#             if int(row_count) > 0:\n",
    "#                 print (\"Pipeline trigger Condition met !!\")\n",
    "#                 submit_pipeline_job()\n",
    "#         else:\n",
    "#             print (\"No pipeline triggered !!!\")\n",
    "\n",
    "# def submit_pipeline_job():\n",
    "#     PROJECT_ID = 'PROJECT_ID'\n",
    "#     REGION = 'REGION'\n",
    "#     BUCKET_NAME = \"BUCKET_NAME\"\n",
    "#     DATASET_NAME = \"mlops\"\n",
    "#     TABLE_NAME = \"chicago\"\n",
    "\n",
    "#     base_output_dir = BUCKET_NAME\n",
    "#     BUCKET_URI = \"gs://{}\".format(BUCKET_NAME)\n",
    "#     PIPELINE_ROOT = \"{}/pipeline_root/chicago-taxi-pipe\".format(BUCKET_URI)\n",
    "#     PIPELINE_NAME = \"vertex-mlops-pipeline-tutorial\"\n",
    "#     EXPERIMENT_NAME = PIPELINE_NAME + \"-experiment\"\n",
    "#     REPO_NAME =\"mlops\"\n",
    "#     TEMPLATE_NAME=\"custom-model-training-evaluation-pipeline\"\n",
    "#     TRAINING_JOB_DISPLAY_NAME=\"taxifare-prediction-training-job\"\n",
    "#     worker_pool_specs = [{\n",
    "#                         \"machine_spec\": {\"machine_type\": \"e2-highmem-2\"},\n",
    "#                         \"replica_count\": 1,\n",
    "#                         \"python_package_spec\":{\n",
    "#                                 \"executor_image_uri\": \"us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest\",\n",
    "#                                 \"package_uris\": [f\"{BUCKET_URI}/trainer-0.1.tar.gz\"],\n",
    "#                                 \"python_module\": \"trainer.task\",\n",
    "#                                 \"args\":[\"--project-id\",PROJECT_ID,\"--training-dir\",f\"/gcs/{BUCKET_NAME}\",\"--bq-source\",f\"{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}\"]\n",
    "#                         },\n",
    "#     }]\n",
    "\n",
    "#     parameters = {\n",
    "#         \"project\": PROJECT_ID,\n",
    "#         \"location\": REGION,\n",
    "#         \"training_job_display_name\": \"taxifare-prediction-training-job\",\n",
    "#         \"worker_pool_specs\": worker_pool_specs,\n",
    "#         \"base_output_dir\": BUCKET_URI,\n",
    "#         \"prediction_container_uri\": \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    "#         \"model_display_name\": \"taxifare-prediction-model\",\n",
    "#         \"batch_prediction_job_display_name\": \"taxifare-prediction-batch-job\",\n",
    "#         \"target_field_name\": \"fare\",\n",
    "#         \"test_data_gcs_uri\": [f\"{BUCKET_URI}/test_no_target.csv\"],\n",
    "#         \"ground_truth_gcs_source\": [f\"{BUCKET_URI}/test.csv\"],\n",
    "#         \"batch_predictions_gcs_prefix\": f\"{BUCKET_URI}/batch_predict_output\",\n",
    "#         \"existing_model\": False\n",
    "#     }\n",
    "#     TEMPLATE_URI = f\"https://{REGION}-kfp.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{TEMPLATE_NAME}/latest\"\n",
    "#     print(\"TEMPLATE URI: \", TEMPLATE_URI)\n",
    "#     request_body = {\n",
    "#         \"name\": PIPELINE_NAME,\n",
    "#         \"displayName\": PIPELINE_NAME,\n",
    "#         \"runtimeConfig\":{\n",
    "#             \"gcsOutputDirectory\": PIPELINE_ROOT,\n",
    "#             \"parameterValues\": parameters,\n",
    "#         },\n",
    "#         \"templateUri\": TEMPLATE_URI\n",
    "#     }\n",
    "#     pipeline_url = \"https://us-central1-aiplatform.googleapis.com/v1/projects/{}/locations/{}/pipelineJobs\".format(PROJECT_ID, REGION)\n",
    "#     creds, project = google.auth.default()\n",
    "#     auth_req = google.auth.transport.requests.Request()\n",
    "#     creds.refresh(auth_req)\n",
    "#     headers = {\n",
    "#     'Authorization': 'Bearer {}'.format(creds.token),\n",
    "#     'Content-Type': 'application/json; charset=utf-8'\n",
    "#     }\n",
    "#     response = requests.request(\"POST\", pipeline_url, headers=headers, data=json.dumps(request_body))\n",
    "#     print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4765beb-737b-4ca6-b0b4-1ae3bf5698c9",
   "metadata": {},
   "source": [
    "### [4] Click **Deploy** to deploy the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880df883-4cc4-4556-8b26-d0419ca6faaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aa065d6-b624-4412-969b-cafc350dc83f",
   "metadata": {},
   "source": [
    "# Add new data to trigger pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dea72aa-c250-4338-bbff-258404d57695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigquery SDK version: 3.25.0\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "print(f'bigquery SDK version: {bigquery.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "273aac1c-2c4c-45af-a86a-b0a0a13065c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"hybrid-vertex\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08890d4-133d-4b60-b97b-846f757821c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSERT INTO `hybrid-vertex.mlops.chicago`\n",
      "(\n",
      "    WITH\n",
      "      taxitrips AS (\n",
      "      SELECT\n",
      "        trip_start_timestamp,\n",
      "        trip_end_timestamp,\n",
      "        trip_seconds,\n",
      "        trip_miles,\n",
      "        payment_type,\n",
      "        pickup_longitude,\n",
      "        pickup_latitude,\n",
      "        dropoff_longitude,\n",
      "        dropoff_latitude,\n",
      "        tips,\n",
      "        tolls,\n",
      "        fare,\n",
      "        pickup_community_area,\n",
      "        dropoff_community_area,\n",
      "        company,\n",
      "        unique_key\n",
      "      FROM\n",
      "        `hybrid-vertex.mlops.taxi_trips`\n",
      "      WHERE pickup_longitude IS NOT NULL\n",
      "      AND pickup_latitude IS NOT NULL\n",
      "      AND dropoff_longitude IS NOT NULL\n",
      "      AND dropoff_latitude IS NOT NULL\n",
      "      AND trip_miles > 0\n",
      "      AND trip_seconds > 0\n",
      "      AND fare > 0\n",
      "      AND EXTRACT(YEAR FROM trip_start_timestamp) = 2022\n",
      "    )\n",
      "\n",
      "    SELECT\n",
      "      trip_start_timestamp,\n",
      "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
      "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
      "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
      "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
      "      trip_seconds,\n",
      "      trip_miles,\n",
      "      payment_type,\n",
      "      ST_AsText(\n",
      "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
      "      ) AS pickup_grid,\n",
      "      ST_AsText(\n",
      "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
      "      ) AS dropoff_grid,\n",
      "      ST_Distance(\n",
      "          ST_GeogPoint(pickup_longitude, pickup_latitude),\n",
      "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
      "      ) AS euclidean,\n",
      "      CONCAT(\n",
      "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
      "              pickup_latitude), 0.1)),\n",
      "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
      "              dropoff_latitude), 0.1))\n",
      "      ) AS loc_cross,\n",
      "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
      "      tips,\n",
      "      tolls,\n",
      "      fare,\n",
      "      pickup_longitude,\n",
      "      pickup_latitude,\n",
      "      dropoff_longitude,\n",
      "      dropoff_latitude,\n",
      "      pickup_community_area,\n",
      "      dropoff_community_area,\n",
      "      company,\n",
      "      unique_key,\n",
      "      trip_end_timestamp\n",
      "    FROM\n",
      "      taxitrips\n",
      "    LIMIT 1000000\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "QUERY = f\"\"\"\n",
    "INSERT INTO `{PROJECT_ID}.mlops.chicago`\n",
    "(\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        trip_start_timestamp,\n",
    "        trip_end_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        tolls,\n",
    "        fare,\n",
    "        pickup_community_area,\n",
    "        dropoff_community_area,\n",
    "        company,\n",
    "        unique_key\n",
    "      FROM\n",
    "        `{PROJECT_ID}.mlops.taxi_trips`\n",
    "      WHERE pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = 2022\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      trip_start_timestamp,\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude),\n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      CONCAT(\n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
    "              pickup_latitude), 0.1)),\n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
    "              dropoff_latitude), 0.1))\n",
    "      ) AS loc_cross,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "      tips,\n",
    "      tolls,\n",
    "      fare,\n",
    "      pickup_longitude,\n",
    "      pickup_latitude,\n",
    "      dropoff_longitude,\n",
    "      dropoff_latitude,\n",
    "      pickup_community_area,\n",
    "      dropoff_community_area,\n",
    "      company,\n",
    "      unique_key,\n",
    "      trip_end_timestamp\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT 1000000\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# print to inspect\n",
    "print(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969a451e-0662-46f8-b03d-543ce0bbce31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.227"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # uncomment to submit\n",
    "job = bq_client.query(query = QUERY)\n",
    "job.result()\n",
    "(job.ended-job.started).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d9c8e-0cc9-4de2-8ac7-cf3342a66bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad277229-edce-47e0-8c6b-6fc60002dc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f0abbe-b225-465b-8f18-eba167596b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f0329-cd4c-439a-aebb-89923471d029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3193ca-8153-4d21-aa47-e1fb86e2609c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7568ad0-b722-446a-b8e5-13c1b0cf44af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653e809-ddab-4184-970b-208c8925dafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
