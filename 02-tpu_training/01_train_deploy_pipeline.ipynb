{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4aabadd-3092-4c3d-bacd-d702055107aa",
   "metadata": {},
   "source": [
    "# Vertex AI Pipeline: train with TPU, register trained model, deploy registered model to endpoint \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcb6681-4144-4771-bd0a-6340fba53b88",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5c093f-7cee-49de-b8a9-ffa484b5b53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import default_config as default\n",
    "\n",
    "# default.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d71ebb-e3b9-4b63-a0a5-23faefdb40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"hybrid-vertex\"\n",
    "LOCATION = \"us-central1\"\n",
    "BUCKET_URI = f\"gs://tpu-pipeline-{PROJECT_ID}\"\n",
    "EMAIL_RECIPIENTS = [ \"jordantotten@google.com\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f9d25a-9911-45f6-9e69-a3b835635bde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 934903580331-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "        \n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01055205-4146-4542-abf6-ef0964e2f8c5",
   "metadata": {},
   "source": [
    "> If your bucket doesn't already exist: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3d3b85-f01e-46eb-b7bf-838fef57fa42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036d8c0-bac3-46f2-aac5-0847f3bddf77",
   "metadata": {},
   "source": [
    "> Set service account access for Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc0aa409-fa7d-40be-9a80-0ca9daa0aa64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://tpu-pipeline-hybrid-vertex/\n",
      "No changes made to gs://tpu-pipeline-hybrid-vertex/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d6deeb-2de2-4032-82b7-47bafea30290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = default.get_config()\n",
    "config.project_id = PROJECT_ID\n",
    "config.location = LOCATION\n",
    "config.bucket_uri = BUCKET_URI\n",
    "config.service_account = SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf71bd-2189-4598-a348-9dd4f77e7a43",
   "metadata": {},
   "source": [
    "## Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dedfaff7-3d76-44bb-9830-c5f2cd5f55a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Any, Dict, List\n",
    "from pprint import pprint\n",
    "\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.custom_job.component import \\\n",
    "    custom_training_job as CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,\n",
    "                                                          ModelDeployOp)\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from kfp import compiler\n",
    "from kfp.dsl import importer_node\n",
    "from kfp import dsl\n",
    "\n",
    "# init vertex SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94acd165-3e02-4614-8383-2816381b13a0",
   "metadata": {},
   "source": [
    "## Set train and deploy compute "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704f77a-04ca-406c-98c9-9366363eeb76",
   "metadata": {},
   "source": [
    "You can set hardware accelerators for both training and prediction:\n",
    "\n",
    "* Set the variables `TRAIN_TPU/TRAIN_NTPU` to use a training container image supporting a TPU and the number of TPUs allocated \n",
    "* Set `DEPLOY_GPU/DEPLOY_NGPU` to use a deployment container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance (otherwise specify `(None, None)` to use a container image to run on a CPU)\n",
    "\n",
    "> *note: TPU VMs don't require VCPU definition*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f02f2-dd56-4be5-a283-945b34f39d70",
   "metadata": {},
   "source": [
    "**accelerators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef42499-03b2-43cd-8fa9-17ba45ee0836",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train compute...\n",
      "  accelerator_type  : TPU_V2\n",
      "  accelerator_count : 8\n",
      "  machine_type      : cloud-tpu\n",
      "  image             : us-central1-docker.pkg.dev/hybrid-vertex/my-tpu-repo/tpu-train:latest\n",
      "\n",
      "Deploy compute...\n",
      "  accelerator_type  : NVIDIA_TESLA_T4\n",
      "  accelerator_count : 1\n",
      "  machine_type      : n1-standard-4\n",
      "  image             : us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-gpu.2-13:latest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accelerators\n",
    "TRAIN_TPU, TRAIN_NTPU = (\n",
    "    gapic.AcceleratorType.TPU_V2,\n",
    "    8,\n",
    ")  # Using TPU_V2 with 8 accelerators\n",
    "DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "    gapic.AcceleratorType.NVIDIA_TESLA_T4,\n",
    "    1,\n",
    ")  # Using Tesla T4 with 1 accelerator\n",
    "\n",
    "# compute\n",
    "TRAIN_MACHINE_TYPE = \"cloud-tpu\"\n",
    "TRAIN_COMPUTE = TRAIN_MACHINE_TYPE\n",
    "DEPLOY_MACHINE_TYPE = \"n1-standard\"\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = DEPLOY_MACHINE_TYPE + \"-\" + VCPU\n",
    "\n",
    "REPOSITORY = \"my-tpu-repo\"\n",
    "\n",
    "# containers\n",
    "IMAGE_NAME = \"tpu-train\"\n",
    "TRAIN_IMAGE = f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE_NAME}:latest\"\n",
    "DEPLOY_VERSION = \"tf2-gpu.2-13\"\n",
    "DEPLOY_IMAGE = f\"us-docker.pkg.dev/cloud-aiplatform/prediction/{DEPLOY_VERSION}:latest\"\n",
    "\n",
    "print(f\"Train compute...\")\n",
    "print(f\"  accelerator_type  : {TRAIN_TPU.name}\")\n",
    "print(f\"  accelerator_count : {TRAIN_NTPU}\")\n",
    "print(f\"  machine_type      : {TRAIN_COMPUTE}\")\n",
    "print(f\"  image             : {TRAIN_IMAGE}\\n\")\n",
    "\n",
    "print(f\"Deploy compute...\")\n",
    "print(f\"  accelerator_type  : {DEPLOY_GPU.name}\")\n",
    "print(f\"  accelerator_count : {DEPLOY_NGPU}\")\n",
    "print(f\"  machine_type      : {DEPLOY_COMPUTE}\")\n",
    "print(f\"  image             : {DEPLOY_IMAGE}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61a3be5c-5c20-492f-a2a6-9a7a4e24a0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update config\n",
    "config.repository = REPOSITORY\n",
    "\n",
    "config.train_tpu = TRAIN_TPU.name\n",
    "config.train_tpu_count = TRAIN_NTPU\n",
    "config.train_compute = TRAIN_COMPUTE\n",
    "config.train_image = TRAIN_IMAGE\n",
    "\n",
    "config.deploy_gpu = DEPLOY_GPU.name\n",
    "config.deploy_gpu_count = DEPLOY_NGPU\n",
    "config.deploy_compute = DEPLOY_COMPUTE\n",
    "config.deploy_image = DEPLOY_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4720b0b-b76e-48df-8731-d0040afc3182",
   "metadata": {},
   "source": [
    "## Training container build artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129fe8d8-6188-452c-a93f-3ccfa330b8cd",
   "metadata": {},
   "source": [
    "#### custom docker image\n",
    "\n",
    "*note: create a directory for writing the container build artifacts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d05170aa-087e-4be5-8d98-6f2c1cb41147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTAINER_ARTIFACTS_DIR = \"tpu-container-artifacts\"\n",
    "\n",
    "! rm -rf {CONTAINER_ARTIFACTS_DIR}\n",
    "! mkdir {CONTAINER_ARTIFACTS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "355b5700-9260-4bb4-950b-f6cb3cbbf812",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tpu-container-artifacts/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile $CONTAINER_ARTIFACTS_DIR/Dockerfile\n",
    "FROM python:3.10\n",
    "\n",
    "WORKDIR /root\n",
    "\n",
    "# Download and install `tensorflow`.\n",
    "RUN pip install https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.13.0/tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "\n",
    "# Download and install `libtpu`.\n",
    "# You must save `libtpu.so` in the '/lib' directory of the container image.\n",
    "RUN curl -L https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.7.0/libtpu.so -o /lib/libtpu.so\n",
    "\n",
    "# Download and install tensorflow-datasets\n",
    "RUN pip3 install tensorflow-datasets tensorboard tensorboard-plugin-profile tensorboard-plugin-wit tensorboard-data-server tensorflow-io\n",
    "RUN pip3 install google-cloud-aiplatform[cloud_profiler]\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY train.py /root/train.py\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49a716-9a0f-413b-8683-4edbdeeea474",
   "metadata": {},
   "source": [
    "#### training script\n",
    "\n",
    "> In the next cell, write the contents of the training script to *train.py*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c657828-c861-443b-bb28-267d59b0c945",
   "metadata": {},
   "source": [
    "In summary, your training script does the following:\n",
    "\n",
    "- Gets the directory where to save the model artifacts from the environment variable `AIP_MODEL_DIR`. This variable is set by the training service.\n",
    "- Loads CIFAR10 dataset from TF Datasets (tfds).\n",
    "- Builds a model using TF.Keras model API.\n",
    "- Compiles the model.\n",
    "- Sets a training distribution strategy according to the argument `args.distribute`.\n",
    "- Trains the model with epochs and steps according to the arguments `args.epochs` and `args.steps`\n",
    "- Saves the trained model to the specified model directory.\n",
    "- Runs TPU specific tasks:\n",
    "    - Finds the TPU cluster, connects to the cluster, and sets the training strategy to TPUStrategy.\n",
    "    - Saves the trained TPU model to the local device so that the model can be saved to the location in `AIP_MODEL_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8200ca1a-02d4-4f08-a9fa-ae1c76a8a1db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tpu-container-artifacts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CONTAINER_ARTIFACTS_DIR}/train.py\n",
    "# Single, Mirror and Multi-Machine Distributed Training for CIFAR-10\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import argparse\n",
    "import os\n",
    "import sys, traceback\n",
    "\n",
    "# gcp\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.training_utils import cloud_profiler\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr', dest='lr',\n",
    "                    default=0.01, type=float,\n",
    "                    help='Learning rate.')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--steps', dest='steps',\n",
    "                    default=200, type=int,\n",
    "                    help='Number of steps per epoch.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='distributed training strategy')\n",
    "parser.add_argument('--batch_size',default=128, \n",
    "                    type=int, help='non-global')\n",
    "parser.add_argument('--project', type=str)\n",
    "parser.add_argument('--location', dest='location',\n",
    "                    default=\"us-central1\", type=str,)\n",
    "parser.add_argument('--tb_instance', type=str)\n",
    "parser.add_argument('--experiment_name', type=str)\n",
    "parser.add_argument('--experiment_run', type=str)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print('Python Version = {}'.format(sys.version))\n",
    "print('TensorFlow Version = {}'.format(tf.__version__))\n",
    "print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "print('DEVICES', device_lib.list_local_devices())\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "# Single Machine, multiple TPU devices\n",
    "elif args.distribute == 'tpu':\n",
    "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n",
    "    tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirror':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "# Multiple Machine, multiple compute device\n",
    "elif args.distribute == 'multi':\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# Multi-worker configuration\n",
    "print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Initialize the profiler.\n",
    "print('Initialize the profiler ...')\n",
    "try:\n",
    "    cloud_profiler.init()\n",
    "except:\n",
    "    ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "    print(\"*** Unexpected:\", ex_type.__name__, ex_value)\n",
    "    traceback.print_tb(ex_traceback, limit=10, file=sys.stdout)\n",
    "print('The profiler initiated.')\n",
    "\n",
    "# initialize Vertex AI sdk\n",
    "aiplatform.init(\n",
    "    project=args.project, \n",
    "    location=args.location,\n",
    "    experiment=args.experiment_name,\n",
    "    experiment_tensorboard=args.tb_instance,\n",
    ")\n",
    "\n",
    "# set job directories\n",
    "MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n",
    "print(f\"MODEL_DIR = {MODEL_DIR}\")\n",
    "\n",
    "log_dir = \"logs\"\n",
    "if 'AIP_TENSORBOARD_LOG_DIR' in os.environ:\n",
    "    log_dir = os.environ['AIP_TENSORBOARD_LOG_DIR']\n",
    "print(f\"log_dir = {log_dir}\")\n",
    "\n",
    "print('Setting up the TensorBoard callback ...')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    update_freq=\"epoch\",\n",
    "    histogram_freq=1,\n",
    "    # embeddings_freq=1,\n",
    "    # profile_batch=20,\n",
    "    # write_graph=True,\n",
    ")\n",
    "\n",
    "# Preparing dataset\n",
    "BUFFER_SIZE = 10000\n",
    "# BATCH_SIZE = 64\n",
    "\n",
    "def make_datasets_unbatched():\n",
    "    # Scaling CIFAR10 data from (0, 255] to (0., 1.]\n",
    "    \n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255.0\n",
    "        return image, label\n",
    "\n",
    "    datasets, info = tfds.load(\n",
    "        name='cifar10', with_info=True, as_supervised=True\n",
    "    )\n",
    "    \n",
    "    return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE).repeat()\n",
    "\n",
    "\n",
    "# Build the Keras model\n",
    "def build_and_compile_cnn_model():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=args.lr),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "# Here the batch size scales up by number of workers since\n",
    "# `tf.data.Dataset.batch` expects the global batch size.\n",
    "GLOBAL_BATCH_SIZE = args.batch_size * NUM_WORKERS\n",
    "\n",
    "print(f\"NUM_WORKERS = {NUM_WORKERS}\")\n",
    "print(f\"BATCH_SIZE  = {args.batch_size}\")\n",
    "print(f\"GLOBAL_BATCH_SIZE = {GLOBAL_BATCH_SIZE}\")\n",
    "\n",
    "train_dataset = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\n",
    "\n",
    "with strategy.scope():\n",
    "    # Creation of dataset, and model building/compiling need to be within\n",
    "    # `strategy.scope()`.\n",
    "    model = build_and_compile_cnn_model()\n",
    "\n",
    "model.fit(\n",
    "    x=train_dataset, \n",
    "    epochs=args.epochs, \n",
    "    steps_per_epoch=args.steps,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "\n",
    "if args.distribute==\"tpu\":\n",
    "    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "    model.save(MODEL_DIR, options=save_locally)\n",
    "else:\n",
    "    model.save(MODEL_DIR)\n",
    "    \n",
    "# print('uploading TB logs ...')\n",
    "# aiplatform.upload_tb_log(\n",
    "#     tensorboard_experiment_name=args.experiment_name,\n",
    "#     logdir=log_dir,\n",
    "#     run_name_prefix=f\"{args.experiment_run}-\",\n",
    "#     allowed_plugins=[\"profile\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b24c404-df38-41c4-b0f9-e353ad05b3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\n",
      "├── 01_train_deploy_pipeline.ipynb\n",
      "├── README.md\n",
      "├── default_config.py\n",
      "├── \u001b[01;34mtpu-container-artifacts\u001b[00m\n",
      "│   ├── Dockerfile\n",
      "│   └── train.py\n",
      "└── tpu_train_cifar10_pipeline.json\n",
      "\n",
      "1 directory, 6 files\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0de2e5-9806-466a-87be-a35afadad711",
   "metadata": {},
   "source": [
    "### Build the training container image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed173b-2bff-46eb-8b39-ac523e73b3b1",
   "metadata": {},
   "source": [
    "Now, build and push the training container image to Artifact Registry using the Dockerfile.\n",
    "\n",
    "In this section, you run the following steps:\n",
    "\n",
    "1. Enable the Artifact Registry API.\n",
    "2. Create a private repository in Artifact Registry.\n",
    "3. Configure authentication to Artifact Registry.\n",
    "4. Submit the training container image using Cloud Build.\n",
    "\n",
    "**Enable Artifact Registry API**\n",
    "\n",
    "> You must enable the Artifact Registry API service for your project.\n",
    "\n",
    "<a href=\"https://cloud.google.com/artifact-registry/docs/enable-service\">Learn more about Enabling service</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78943666-f21f-4e2d-b83e-118deb7bdc4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Create request issued for: [my-tpu-repo]\n",
      "Waiting for operation [projects/hybrid-vertex/locations/us-central1/operations/\n",
      "7ede907a-43c3-4b44-925e-3a7d07ac7d0a] to complete...done.                      \n",
      "Created repository [my-tpu-repo].\n"
     ]
    }
   ],
   "source": [
    "# ! gcloud config set project {PROJECT_ID}\n",
    "# ! gcloud services enable artifactregistry.googleapis.com\n",
    "\n",
    "# if os.getenv(\"IS_TESTING\"):\n",
    "#     ! sudo apt-get update --yes && sudo apt-get --only-upgrade --yes install google-cloud-sdk-cloud-run-proxy google-cloud-sdk-harbourbridge google-cloud-sdk-cbt google-cloud-sdk-gke-gcloud-auth-plugin google-cloud-sdk-kpt google-cloud-sdk-local-extract google-cloud-sdk-minikube google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-app-engine-python google-cloud-sdk-spanner-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-nomos google-cloud-sdk-package-go-module google-cloud-sdk-firestore-emulator kubectl google-cloud-sdk-datastore-emulator google-cloud-sdk-app-engine-python-extras google-cloud-sdk-cloud-build-local google-cloud-sdk-kubectl-oidc google-cloud-sdk-anthos-auth google-cloud-sdk-app-engine-grpc google-cloud-sdk-pubsub-emulator google-cloud-sdk-datalab google-cloud-sdk-skaffold google-cloud-sdk google-cloud-sdk-terraform-tools google-cloud-sdk-config-connector\n",
    "#     ! gcloud components update --quiet\n",
    "\n",
    "# # Create the repository (only run once)\n",
    "# ! gcloud artifacts repositories create $REPOSITORY --repository-format=docker --location=$LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "997f3d29-0a82-469d-9b95-769c5da46441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth configure-docker $LOCATION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eda7f9-acdd-4df6-8745-dbe8471f2999",
   "metadata": {},
   "source": [
    "## Submit container to Cloud Build\n",
    "\n",
    "> Submit the training container image using Cloud Build. The image gets saved to the repository path that is provided in the tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2ab6701-2e78-425b-9c50-b57451909835",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 2 file(s) totalling 6.5 KiB before compression.\n",
      "Uploading tarball of [tpu-container-artifacts] to [gs://hybrid-vertex_cloudbuild/source/1736824521.534143-89c23a59fee74575bcb7284c35e10d55.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/us-central1/builds/100a219e-e3d7-4bd5-938f-8bf65a2da9b6].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/100a219e-e3d7-4bd5-938f-8bf65a2da9b6?project=934903580331 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"100a219e-e3d7-4bd5-938f-8bf65a2da9b6\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1736824521.534143-89c23a59fee74575bcb7284c35e10d55.tgz#1736824521814929\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1736824521.534143-89c23a59fee74575bcb7284c35e10d55.tgz#1736824521814929...\n",
      "/ [1 files][  2.7 KiB/  2.7 KiB]                                                \n",
      "Operation completed over 1 objects/2.7 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  9.216kB\n",
      "Step 1/8 : FROM python:3.10\n",
      "3.10: Pulling from library/python\n",
      "0a96bdb82805: Already exists\n",
      "54c7be425079: Already exists\n",
      "7aa8176e6d89: Already exists\n",
      "1523f4b3f560: Already exists\n",
      "83ec4e83e66b: Pulling fs layer\n",
      "ee8db358fbe4: Pulling fs layer\n",
      "ddfcc6240e72: Pulling fs layer\n",
      "ddfcc6240e72: Verifying Checksum\n",
      "ddfcc6240e72: Download complete\n",
      "83ec4e83e66b: Verifying Checksum\n",
      "83ec4e83e66b: Download complete\n",
      "ee8db358fbe4: Verifying Checksum\n",
      "ee8db358fbe4: Download complete\n",
      "83ec4e83e66b: Pull complete\n",
      "ee8db358fbe4: Pull complete\n",
      "ddfcc6240e72: Pull complete\n",
      "Digest: sha256:81b81c80d41ec59dcee2c373b8e1d73a0b6949df793db1b043a033ca6837e02d\n",
      "Status: Downloaded newer image for python:3.10\n",
      " ---> ab7315e562dd\n",
      "Step 2/8 : WORKDIR /root\n",
      " ---> Running in e4a4da2408fd\n",
      "Removing intermediate container e4a4da2408fd\n",
      " ---> 09c69a96e82c\n",
      "Step 3/8 : RUN pip install https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.13.0/tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      " ---> Running in 8ad618c3474e\n",
      "Collecting tensorflow==2.13.0\n",
      "  Downloading https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.13.0/tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.3/225.3 MB 7.7 MB/s eta 0:00:00\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.6/294.6 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 25.7 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 12.4 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/24.5 MB 69.3 MB/s eta 0:00:00\n",
      "Collecting numpy<=1.24.3,>=1.22\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 81.9 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/5.1 MB 82.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow==2.13.0) (65.5.1)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.17.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.8/82.8 kB 18.1 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.69.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 111.7 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.9/71.9 kB 15.2 MB/s eta 0:00:00\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 82.1 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 94.4 MB/s eta 0:00:00\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 96.5 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 440.8/440.8 kB 56.4 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 12.7 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.45.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.3/106.3 kB 22.3 MB/s eta 0:00:00\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 13.6 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.8/209.8 kB 37.4 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 34.4 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 93.0 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.5/181.5 kB 31.6 MB/s eta 0:00:00\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.4/128.4 kB 24.8 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164.9/164.9 kB 30.2 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.4/70.4 kB 13.8 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146.1/146.1 kB 29.2 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.1/83.1 kB 17.8 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 27.9 MB/s eta 0:00:00\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, pyasn1, protobuf, packaging, opt-einsum, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, h5py, google-pasta, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.5.0 certifi-2024.12.14 charset-normalizer-3.4.1 flatbuffers-24.12.23 gast-0.4.0 google-auth-2.37.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.69.0 h5py-3.12.1 idna-3.10 keras-2.13.1 libclang-18.1.1 markdown-3.7 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.4.0 packaging-24.2 protobuf-4.25.5 pyasn1-0.6.1 pyasn1-modules-0.4.1 requests-2.32.3 requests-oauthlib-2.0.0 rsa-4.9 six-1.17.0 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 typing-extensions-4.5.0 urllib3-2.3.0 werkzeug-3.1.3 wrapt-1.17.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 8ad618c3474e\n",
      " ---> b336203d228f\n",
      "Step 4/8 : RUN curl -L https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.7.0/libtpu.so -o /lib/libtpu.so\n",
      " ---> Running in b5d2bc95c427\n",
      "\u001b[91m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  188M  100  188M    0     0   180M      0  0:00:01  0:00:01 --:--:--  180M\u001b[0m\u001b[91m\n",
      "\u001b[0mRemoving intermediate container b5d2bc95c427\n",
      " ---> a801d1c7b028\n",
      "Step 5/8 : RUN pip3 install tensorflow-datasets tensorboard tensorboard-plugin-profile tensorboard-plugin-wit tensorboard-data-server tensorflow-io\n",
      " ---> Running in ae614ff07365\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.7-py3-none-any.whl (5.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 45.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/site-packages (2.13.0)\n",
      "Collecting tensorboard-plugin-profile\n",
      "  Downloading tensorboard_plugin_profile-2.18.0-py3-none-any.whl (5.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/5.7 MB 99.2 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 62.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorboard-data-server in /usr/local/lib/python3.10/site-packages (0.7.2)\n",
      "Collecting tensorflow-io\n",
      "  Downloading tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.6/49.6 MB 37.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (2.1.0)\n",
      "Collecting simple-parsing\n",
      "  Downloading simple_parsing-0.1.6-py3-none-any.whl (112 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.6/112.6 kB 20.5 MB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.2/98.2 kB 18.2 MB/s eta 0:00:00\n",
      "Collecting psutil\n",
      "  Downloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 287.5/287.5 kB 41.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (1.24.3)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.8/152.8 kB 29.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (4.25.5)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (2.5.0)\n",
      "Collecting array-record>=0.5.0\n",
      "  Downloading array_record-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 57.5 MB/s eta 0:00:00\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.1/40.1 MB 46.1 MB/s eta 0:00:00\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 15.5 MB/s eta 0:00:00\n",
      "Collecting etils[edc,enp,epath,epy,etree]>=1.6.0\n",
      "  Downloading etils-1.11.0-py3-none-any.whl (165 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 165.4/165.4 kB 24.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (1.17.1)\n",
      "Collecting immutabledict\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/site-packages (from tensorboard) (2.37.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/site-packages (from tensorboard) (1.69.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/site-packages (from tensorboard) (0.45.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/site-packages (from tensorboard) (65.5.1)\n",
      "Collecting gviz-api>=1.9.0\n",
      "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/site-packages (from tensorboard-plugin-profile) (1.17.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.37.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-io) (0.37.1)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183.9/183.9 kB 32.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0->tensorflow-datasets) (4.5.0)\n",
      "Collecting zipp\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Collecting importlib_resources\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.5.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.12.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Collecting docstring-parser<1.0,>=0.15\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Collecting protobuf>=3.20\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 78.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21484 sha256=5a702e5cec41366b335cd5ed2e4b670a360ff85adfca97550070684be7372792\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/4e/28/3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
      "Successfully built promise\n",
      "Installing collected packages: tensorboard-plugin-wit, dm-tree, zipp, tqdm, toml, tensorflow-io, pyarrow, psutil, protobuf, promise, importlib_resources, immutabledict, gviz-api, fsspec, etils, docstring-parser, click, tensorflow-metadata, tensorboard-plugin-profile, simple-parsing, array-record, tensorflow-datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.5\n",
      "    Uninstalling protobuf-4.25.5:\n",
      "      Successfully uninstalled protobuf-4.25.5\n",
      "Successfully installed array-record-0.6.0 click-8.1.8 dm-tree-0.1.8 docstring-parser-0.16 etils-1.11.0 fsspec-2024.12.0 gviz-api-1.10.0 immutabledict-4.2.1 importlib_resources-6.5.2 promise-2.3 protobuf-3.20.3 psutil-6.1.1 pyarrow-18.1.0 simple-parsing-0.1.6 tensorboard-plugin-profile-2.18.0 tensorboard-plugin-wit-1.8.1 tensorflow-datasets-4.9.7 tensorflow-io-0.37.1 tensorflow-metadata-1.16.1 toml-0.10.2 tqdm-4.67.1 zipp-3.21.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container ae614ff07365\n",
      " ---> 98ec33c6555b\n",
      "Step 6/8 : RUN pip3 install google-cloud-aiplatform[cloud_profiler]\n",
      " ---> Running in d47f4634c719\n",
      "Collecting google-cloud-aiplatform[cloud_profiler]\n",
      "  Downloading google_cloud_aiplatform-1.76.0-py2.py3-none-any.whl (6.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.9/6.9 MB 54.5 MB/s eta 0:00:00\n",
      "Collecting shapely<3.0.0dev\n",
      "  Downloading shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 85.9 MB/s eta 0:00:00\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.19.0-py2.py3-none-any.whl (131 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.8/131.8 kB 24.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/site-packages (from google-cloud-aiplatform[cloud_profiler]) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from google-cloud-aiplatform[cloud_profiler]) (4.5.0)\n",
      "Collecting google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-3.27.0-py2.py3-none-any.whl (240 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.1/240.1 kB 39.6 MB/s eta 0:00:00\n",
      "Collecting pydantic<3\n",
      "  Downloading pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 431.4/431.4 kB 54.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/site-packages (from google-cloud-aiplatform[cloud_profiler]) (24.2)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1\n",
      "  Downloading google_api_core-2.24.0-py3-none-any.whl (158 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.6/158.6 kB 28.8 MB/s eta 0:00:00\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.14.0-py2.py3-none-any.whl (384 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 384.1/384.1 kB 49.4 MB/s eta 0:00:00\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.1/50.1 kB 10.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/site-packages (from google-cloud-aiplatform[cloud_profiler]) (2.37.0)\n",
      "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/site-packages (from google-cloud-aiplatform[cloud_profiler]) (0.16)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 221.7/221.7 kB 37.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform[cloud_profiler]) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform[cloud_profiler]) (1.69.0)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "  Downloading grpcio_status-1.69.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform[cloud_profiler]) (5.5.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform[cloud_profiler]) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform[cloud_profiler]) (0.4.1)\n",
      "Collecting google-resumable-media<3.0dev,>=2.0.0\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.3/81.3 kB 16.6 MB/s eta 0:00:00\n",
      "Collecting python-dateutil<3.0dev,>=2.7.3\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 38.7 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0.0dev,>=2.4.1\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.14.0-py2.py3-none-any.whl (27 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37 kB)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 92.1 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform[cloud_profiler]) (1.24.3)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 319.7/319.7 kB 49.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform[cloud_profiler]) (0.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (1.17.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform[cloud_profiler]) (2024.12.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform[cloud_profiler]) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform[cloud_profiler]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform[cloud_profiler]) (3.10)\n",
      "Installing collected packages: typing-extensions, shapely, python-dateutil, protobuf, google-crc32c, annotated-types, pydantic-core, proto-plus, googleapis-common-protos, google-resumable-media, pydantic, grpcio-status, google-api-core, grpc-google-iam-v1, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\n",
      "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\n",
      "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.29.3 which is incompatible.\n",
      "tensorboard-plugin-profile 2.18.0 requires protobuf<5.0.0dev,>=3.19.6, but you have protobuf 5.29.3 which is incompatible.\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 google-api-core-2.24.0 google-cloud-aiplatform-1.76.0 google-cloud-bigquery-3.27.0 google-cloud-core-2.4.1 google-cloud-resource-manager-1.14.0 google-cloud-storage-2.19.0 google-crc32c-1.6.0 google-resumable-media-2.7.2 googleapis-common-protos-1.66.0 grpc-google-iam-v1-0.14.0 grpcio-status-1.69.0 proto-plus-1.25.0 protobuf-5.29.3 pydantic-2.10.5 pydantic-core-2.27.2 python-dateutil-2.9.0.post0 shapely-2.0.6 typing-extensions-4.12.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container d47f4634c719\n",
      " ---> 33173c65d547\n",
      "Step 7/8 : COPY train.py /root/train.py\n",
      " ---> 94db42a5a331\n",
      "Step 8/8 : ENTRYPOINT [\"python3\", \"train.py\"]\n",
      " ---> Running in 71cb3af04bb9\n",
      "Removing intermediate container 71cb3af04bb9\n",
      " ---> 693381293c30\n",
      "Successfully built 693381293c30\n",
      "Successfully tagged us-central1-docker.pkg.dev/hybrid-vertex/my-tpu-repo/tpu-train:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/hybrid-vertex/my-tpu-repo/tpu-train:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/hybrid-vertex/my-tpu-repo/tpu-train]\n",
      "de7326665ee4: Preparing\n",
      "80557537a8dc: Preparing\n",
      "44b8a4508a51: Preparing\n",
      "63f3d0ab0633: Preparing\n",
      "dbfcd1af3f2a: Preparing\n",
      "e604bce64948: Preparing\n",
      "8b5a701e8431: Preparing\n",
      "14ebb3438d1a: Preparing\n",
      "6d58389117c3: Preparing\n",
      "85c6f0cfb532: Preparing\n",
      "a4fd1e7df47e: Preparing\n",
      "2f7b6d216a37: Preparing\n",
      "e604bce64948: Waiting\n",
      "8b5a701e8431: Waiting\n",
      "14ebb3438d1a: Waiting\n",
      "6d58389117c3: Waiting\n",
      "85c6f0cfb532: Waiting\n",
      "a4fd1e7df47e: Waiting\n",
      "2f7b6d216a37: Waiting\n",
      "de7326665ee4: Pushed\n",
      "e604bce64948: Layer already exists\n",
      "8b5a701e8431: Layer already exists\n",
      "14ebb3438d1a: Layer already exists\n",
      "6d58389117c3: Layer already exists\n",
      "85c6f0cfb532: Layer already exists\n",
      "a4fd1e7df47e: Layer already exists\n",
      "63f3d0ab0633: Pushed\n",
      "80557537a8dc: Pushed\n",
      "2f7b6d216a37: Pushed\n",
      "44b8a4508a51: Pushed\n",
      "dbfcd1af3f2a: Pushed\n",
      "latest: digest: sha256:9c49a0a55e6992b4248543b013f270cccb7428430f74fb563eb8c53e2a8f999f size: 2854\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                                    STATUS\n",
      "100a219e-e3d7-4bd5-938f-8bf65a2da9b6  2025-01-14T03:15:22+00:00  3M24S     gs://hybrid-vertex_cloudbuild/source/1736824521.534143-89c23a59fee74575bcb7284c35e10d55.tgz  us-central1-docker.pkg.dev/hybrid-vertex/my-tpu-repo/tpu-train (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit {CONTAINER_ARTIFACTS_DIR} --region={LOCATION} --tag={TRAIN_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5becbc-d0c8-4713-8365-f50976626016",
   "metadata": {},
   "source": [
    "# Build pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87167d17-0cdd-40f9-a28c-05e9c19ddaf1",
   "metadata": {},
   "source": [
    "## Set Vertex Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10cf5399-b18d-4d42-a3cd-75900339f18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXP_VERSION=\"v11\"\n",
    "RUN_TAG = \"b128\" # b128 | b256 | b512 | b1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2883b598-8515-4c34-9930-042d9ce78dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-0d9c9155-ee03-441d-b3a1-84ce2680b5b7\" href=\"#view-view-vertex-resource-0d9c9155-ee03-441d-b3a1-84ce2680b5b7\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-0d9c9155-ee03-441d-b3a1-84ce2680b5b7');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/tpu-train-v11/runs?project=hybrid-vertex');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/tpu-train-v11/runs?project=hybrid-vertex', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME : tpu-train-v11\n",
      "RUN_NAME        : run-b128\n",
      "\n",
      "CHECKPT_DIR     : gs://tpu-pipeline-hybrid-vertex/tpu-train-v11/chkpoint\n",
      "BASE_OUTPUT_DIR : gs://tpu-pipeline-hybrid-vertex/tpu-train-v11/run-b128\n",
      "LOG_DIR         : gs://tpu-pipeline-hybrid-vertex/tpu-train-v11/run-b128/logs\n",
      "DATA_DIR        : gs://tpu-pipeline-hybrid-vertex/tpu-train-v11/run-b128/data\n",
      "ARTIFACTS_DIR   : gs://tpu-pipeline-hybrid-vertex/tpu-train-v11/run-b128/model\n",
      "\n",
      "TB_INSTANCE     : projects/934903580331/locations/us-central1/tensorboards/257140585364717568\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME   = f'tpu-train-{EXP_VERSION}'\n",
    "invoke_time       = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# new experiment run\n",
    "if RUN_TAG:\n",
    "    RUN_NAME      = f'run-{RUN_TAG}'\n",
    "else:\n",
    "    RUN_NAME      = f'run-{invoke_time}'\n",
    "\n",
    "# output dirs\n",
    "EXPERIMENT_DIR    = os.path.join(BUCKET_URI, EXPERIMENT_NAME)\n",
    "CHECKPT_DIR       = os.path.join(EXPERIMENT_DIR, \"chkpoint\")\n",
    "BASE_OUTPUT_DIR   = os.path.join(EXPERIMENT_DIR, RUN_NAME)\n",
    "LOG_DIR           = os.path.join(BASE_OUTPUT_DIR, \"logs\")\n",
    "DATA_DIR          = os.path.join(BASE_OUTPUT_DIR, \"data\")\n",
    "ARTIFACTS_DIR     = os.path.join(BASE_OUTPUT_DIR, \"model\")\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    # staging_bucket=BUCKET_URI,\n",
    "    location=LOCATION,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    experiment_tensorboard=True,\n",
    ")\n",
    "\n",
    "tensorboard = aiplatform.Experiment(EXPERIMENT_NAME).get_backing_tensorboard_resource()\n",
    "TB_INSTANCE = tensorboard.resource_name\n",
    "\n",
    "print(f\"EXPERIMENT_NAME : {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME        : {RUN_NAME}\\n\")\n",
    "print(f\"CHECKPT_DIR     : {CHECKPT_DIR}\")\n",
    "print(f\"BASE_OUTPUT_DIR : {BASE_OUTPUT_DIR}\")\n",
    "print(f\"LOG_DIR         : {LOG_DIR}\")\n",
    "print(f\"DATA_DIR        : {DATA_DIR}\")\n",
    "print(f\"ARTIFACTS_DIR   : {ARTIFACTS_DIR}\\n\")\n",
    "print(f\"TB_INSTANCE     : {TB_INSTANCE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "242f423f-e1b1-4d01-8643-872106a99be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config.experiment_name = EXPERIMENT_NAME\n",
    "# config.run_name = RUN_NAME\n",
    "# config.experiment_dir = EXPERIMENT_DIR\n",
    "# config.checkpoint_dir = CHECKPT_DIR\n",
    "# config.base_output_dir = BASE_OUTPUT_DIR\n",
    "# config.log_dir = LOG_DIR\n",
    "# config.data_dir = DATA_DIR\n",
    "# config.artifacts_dir = ARTIFACTS_DIR\n",
    "config.tb_instance = TB_INSTANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bd069-c567-49b1-8e92-2d32244e5a84",
   "metadata": {},
   "source": [
    "## Define pipeline\n",
    "\n",
    "> The components required for the key tasks of the pipeline are defined using  [`google_cloud_pipeline_components`](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud). These tasks involve: upload the model, create an endpoint, and deploy the model to the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854934c-3c56-4bab-895d-aec6a6a34b23",
   "metadata": {},
   "source": [
    "The pipeline has four main steps:\n",
    "\n",
    "1) The `CustomTrainingJobOp` runs the docker container image which executes the training task using TPU environment.\n",
    "2) The `ModelUploadOp` uploads the trained model to Vertex AI Model Registry.\n",
    "3) The `EndpointCreateOp` creates a Vertex AI endpoint resource.\n",
    "4) Finally, the `ModelDeployOp` deploys the model to the endpoint.\n",
    "\n",
    "**Note:** The `ModelDeployOp` component creates an endpoint if one isn't provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64a5cdce-67de-487c-8f9f-d2b9b83dbde1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT      : gs://tpu-pipeline-hybrid-vertex/tpu-train-v11/pipeline_root/tpu_cifar10_pipeline\n",
      "PIPELINE_NAME      : train-endpoint-deploy-v11\n",
      "MODEL_DISPLAY_NAME : cifar10-model-run-b128\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_ROOT = f\"{EXPERIMENT_DIR}/pipeline_root/tpu_cifar10_pipeline\"\n",
    "PIPELINE_NAME = f\"train-endpoint-deploy-{EXP_VERSION}\"\n",
    "\n",
    "MODEL_DISPLAY_NAME = f\"cifar10-model-{RUN_NAME}\"\n",
    "\n",
    "# update config\n",
    "config.pipeline_root = PIPELINE_ROOT\n",
    "config.pipeline_name = PIPELINE_NAME\n",
    "config.model_display_name = MODEL_DISPLAY_NAME\n",
    "\n",
    "print(f\"PIPELINE_ROOT      : {PIPELINE_ROOT}\")\n",
    "print(f\"PIPELINE_NAME      : {PIPELINE_NAME}\")\n",
    "print(f\"MODEL_DISPLAY_NAME : {MODEL_DISPLAY_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deb22d46-1864-4714-8d37-eb9745d64e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import importer\n",
    "# from kfp.dsl import OneOf\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# Define the pipeline\n",
    "@kfp.dsl.pipeline(name=PIPELINE_NAME)\n",
    "def pipeline(\n",
    "    worker_pool_specs: list,\n",
    "    model_display_name: str,\n",
    "    serving_container_image_uri: str,\n",
    "    model_artifact_uri: str,\n",
    "    deployment_machine_type: str,\n",
    "    deployment_accelerator_type: str,\n",
    "    deployment_accelerator_count: int,\n",
    "    project: str,\n",
    "    tensorboard_resource: str,\n",
    "    service_account: str,\n",
    "    existing_endpoint: bool = False,\n",
    "    endpoint_resource_uri: str = None,\n",
    "    endpoint_resource_name: str = None,\n",
    "    base_output_directory: str = None,\n",
    "    # experiment_name: str = None,\n",
    "    # experiment_run: str = None,\n",
    "):\n",
    "    \n",
    "    # Notification task\n",
    "    notify_task = VertexNotificationEmailOp(\n",
    "        recipients= EMAIL_RECIPIENTS\n",
    "    )\n",
    "    \n",
    "    with dsl.ExitHandler(notify_task, name='TPU Train Pipeline'):\n",
    "\n",
    "    # Run the custom training job\n",
    "    custom_job_task = CustomTrainingJobOp(\n",
    "        display_name=model_display_name,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        base_output_directory=base_output_directory,\n",
    "        tensorboard=tensorboard_resource,\n",
    "        service_account=service_account,\n",
    "        enable_web_access=True,\n",
    "    )\n",
    "\n",
    "    # # Import the trained model\n",
    "    # import_unmanaged_model_task = importer_node.importer(\n",
    "    #     artifact_uri=model_artifact_uri,\n",
    "    #     artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "    #     metadata={\n",
    "    #         \"containerSpec\": {\"imageUri\": serving_container_image_uri},\n",
    "    #     },\n",
    "    # ).after(custom_job_task)\n",
    "    \n",
    "    # Import the trained model\n",
    "    import_unmanaged_model_task = importer(\n",
    "        artifact_uri=model_artifact_uri,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\"imageUri\": serving_container_image_uri},\n",
    "        },\n",
    "    ).after(custom_job_task)\n",
    "\n",
    "    # Upload the model\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=project,\n",
    "        display_name=model_display_name,\n",
    "        unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "    )\n",
    "    \n",
    "    with dsl.If(existing_endpoint == True):\n",
    "        \n",
    "        # Import existing endpoint\n",
    "        endpoint = importer(\n",
    "            artifact_uri=endpoint_resource_uri,\n",
    "            artifact_class=artifact_types.VertexEndpoint,\n",
    "            metadata={\"resourceName\": endpoint_resource_name},\n",
    "        )\n",
    "        # Deploy model to existing endpoint\n",
    "        _ = ModelDeployOp(\n",
    "            endpoint=endpoint.output,\n",
    "            model=model_upload_op.outputs[\"model\"],\n",
    "            deployed_model_display_name=model_display_name,\n",
    "            dedicated_resources_machine_type=deployment_machine_type,\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            dedicated_resources_accelerator_type=deployment_accelerator_type,\n",
    "            dedicated_resources_accelerator_count=deployment_accelerator_count,\n",
    "            traffic_split={\"0\": 100},\n",
    "        )\n",
    "        \n",
    "    with dsl.Else():\n",
    "        # Create an endpoint\n",
    "        endpoint_create_op = EndpointCreateOp(\n",
    "            project=project,\n",
    "            display_name=\"tpu-pipeline-created-endpoint\",\n",
    "        )\n",
    "\n",
    "        # Deploy the model to new endpoint\n",
    "        _ = ModelDeployOp(\n",
    "            endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "            model=model_upload_op.outputs[\"model\"],\n",
    "            deployed_model_display_name=model_display_name,\n",
    "            dedicated_resources_machine_type=deployment_machine_type,\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            dedicated_resources_accelerator_type=deployment_accelerator_type,\n",
    "            dedicated_resources_accelerator_count=deployment_accelerator_count,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb736f4b-0e33-422e-8e6f-2b3bf3582f6b",
   "metadata": {},
   "source": [
    "## Compile the pipeline\n",
    "\n",
    "Next, compile the pipeline to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc7781b0-1e1c-4877-8c4d-e68f66e22ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the file name\n",
    "PIPELINE_PACKAGE_FILE = \"tpu_train_cifar10_pipeline.json\"\n",
    "\n",
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=PIPELINE_PACKAGE_FILE,\n",
    ")\n",
    "\n",
    "config.pipeline_local_json = PIPELINE_PACKAGE_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268197d-81d4-4ca9-b466-da65cc472276",
   "metadata": {},
   "source": [
    "## Run the pipeline\n",
    "\n",
    "Before you run the pipeline, define worker pool specs and other parameters required for running the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bd0aef-0f7c-45f0-8fbf-6b8787e2e6cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "The above defined pipeline takes the following parameters:\n",
    "\n",
    "- `worker_pool_specs`: The worker pool specs required for running the training job.\n",
    "- `model_display_name`: A display name for the uploaded model.\n",
    "- `serving_container_image_uri`: Container image used for model deployment.\n",
    "- `model_artifact_uri`: Artifact location of the trained model.\n",
    "- `deployment_machine_type`: Machine type for model deployment.\n",
    "- `deployment_accelerator_type`: Accelerator type for model deployment.\n",
    "- `deployment_accelerator_count`: Number of accelerators required for model deployment.\n",
    "- `project`: ID of the Google Cloud project where the pipeline runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9adb21-f1ab-4cd7-842f-068b78901ed5",
   "metadata": {},
   "source": [
    "### Set train args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "202351b4-318b-4a4d-805f-17439bc8756e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--epochs=10',\n",
      "                              '--steps=10000',\n",
      "                              '--distribute=tpu',\n",
      "                              '--project=hybrid-vertex',\n",
      "                              '--tb_instance=projects/934903580331/locations/us-central1/tensorboards/257140585364717568',\n",
      "                              '--experiment_name=tpu-train-v11',\n",
      "                              '--experiment_run=run-b128',\n",
      "                              '--batch_size=128'],\n",
      "                     'env': [{'name': 'AIP_MODEL_DIR',\n",
      "                              'value': 'gs://tpu-pipeline-hybrid-vertex/tpu-train-v11/run-b128/model'},\n",
      "                             {'name': 'AIP_TENSORBOARD_LOG_DIR',\n",
      "                              'value': 'gs://tpu-pipeline-hybrid-vertex/tpu-train-v11/run-b128/logs'}],\n",
      "                     'image_uri': 'us-central1-docker.pkg.dev/hybrid-vertex/my-tpu-repo/tpu-train:latest'},\n",
      "  'machine_spec': {'accelerator_count': 8,\n",
      "                   'accelerator_type': <AcceleratorType.TPU_V2: 6>,\n",
      "                   'machine_type': 'cloud-tpu'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "STEPS = 10_000\n",
    "BATCH_SIZE = 128 # 128 256 512 1024\n",
    "\n",
    "if not TRAIN_NTPU or TRAIN_NTPU < 2:\n",
    "    TRAIN_STRATEGY = \"single\"\n",
    "else:\n",
    "    TRAIN_STRATEGY = \"tpu\"\n",
    "\n",
    "TRAINER_ARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--steps=\" + str(STEPS),\n",
    "    \"--distribute=\" + TRAIN_STRATEGY,\n",
    "    \"--project=\" + PROJECT_ID,\n",
    "    \"--location=\" + LOCATION,\n",
    "    \"--tb_instance=\" + TB_INSTANCE,\n",
    "    \"--experiment_name=\" + EXPERIMENT_NAME,\n",
    "    \"--experiment_run=\" + RUN_NAME,\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "]\n",
    "\n",
    "# Define the worker pool specs required for custom training job\n",
    "WORKER_POOL_SPECS = [\n",
    "    {\n",
    "        \"container_spec\": {\n",
    "            \"args\": TRAINER_ARGS,\n",
    "            \"env\": [\n",
    "                {\"name\": \"AIP_MODEL_DIR\", \"value\": ARTIFACTS_DIR},\n",
    "                {\"name\": \"AIP_TENSORBOARD_LOG_DIR\", \"value\": LOG_DIR}\n",
    "            ],\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": TRAIN_COMPUTE,\n",
    "            \"accelerator_type\": TRAIN_TPU,\n",
    "            \"accelerator_count\": TRAIN_NTPU,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db62c15-e7d6-441b-93d7-e040a7b62e8d",
   "metadata": {},
   "source": [
    "### set deploy args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce77d2d0-7a88-4071-b409-62e18eb8213a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXISTING_ENDPOINT_BOOL : True\n",
      "ENDPOINT_RESOURCE_NAME : projects/934903580331/locations/us-central1/endpoints/4267284891747483648\n",
      "ENDPOINT_RESOURCE_URI  : https://us-central1-aiplatform.googleapis.com/v1/projects/934903580331/locations/us-central1/endpoints/4267284891747483648\n"
     ]
    }
   ],
   "source": [
    "# existing endpoint \n",
    "EXISTING_ENDPOINT_BOOL = True\n",
    "ENDPOINT_ID = \"4267284891747483648\"\n",
    "ENDPOINT_RESOURCE_NAME=f\"projects/934903580331/locations/us-central1/endpoints/{ENDPOINT_ID}\"\n",
    "ENDPOINT_RESOURCE_URI = f\"https://us-central1-aiplatform.googleapis.com/v1/{ENDPOINT_RESOURCE_NAME}\"\n",
    "\n",
    "if not EXISTING_ENDPOINT_BOOL:\n",
    "    ENDPOINT_RESOURCE_NAME = None\n",
    "    ENDPOINT_RESOURCE_URI = None\n",
    "\n",
    "print(f\"EXISTING_ENDPOINT_BOOL : {EXISTING_ENDPOINT_BOOL}\")\n",
    "print(f\"ENDPOINT_RESOURCE_NAME : {ENDPOINT_RESOURCE_NAME}\")\n",
    "print(f\"ENDPOINT_RESOURCE_URI  : {ENDPOINT_RESOURCE_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dfc2f9-ed86-4761-90f4-b77c0491b040",
   "metadata": {},
   "source": [
    "### update config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a051667-e66e-4742-912d-4b571db19ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config.train_strategy = TRAIN_STRATEGY\n",
    "config.epochs = EPOCHS\n",
    "config.steps = STEPS\n",
    "config.trainer_args = TRAINER_ARGS\n",
    "config.endpoint_id = ENDPOINT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691002ac-979e-4a4e-a2cb-c7c38866eca1",
   "metadata": {},
   "source": [
    "## submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e0cdb36-26b9-408b-b26d-a703c6043c63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/train-endpoint-deploy-v11-20250109173831\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/train-endpoint-deploy-v11-20250109173831')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/train-endpoint-deploy-v11-20250109173831?project=934903580331\n",
      "Associating projects/934903580331/locations/us-central1/pipelineJobs/train-endpoint-deploy-v11-20250109173831 to Experiment: tpu-train-v11\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINE_PACKAGE_FILE,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"worker_pool_specs\": WORKER_POOL_SPECS,\n",
    "        \"model_display_name\": MODEL_DISPLAY_NAME,\n",
    "        \"serving_container_image_uri\": DEPLOY_IMAGE,\n",
    "        \"model_artifact_uri\": ARTIFACTS_DIR,\n",
    "        \"deployment_machine_type\": DEPLOY_COMPUTE,\n",
    "        \"deployment_accelerator_type\": DEPLOY_GPU.name,\n",
    "        \"deployment_accelerator_count\": DEPLOY_NGPU,\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"tensorboard_resource\": TB_INSTANCE,\n",
    "        \"service_account\": SERVICE_ACCOUNT,\n",
    "        \"existing_endpoint\": EXISTING_ENDPOINT_BOOL,\n",
    "        \"endpoint_resource_uri\": ENDPOINT_RESOURCE_URI,\n",
    "        \"endpoint_resource_name\": ENDPOINT_RESOURCE_NAME,\n",
    "        \"base_output_directory\": BASE_OUTPUT_DIR,\n",
    "        # \"experiment_name\": EXPERIMENT_NAME,\n",
    "        # \"experiment_run\": RUN_NAME,\n",
    "    },\n",
    ")\n",
    "\n",
    "# # Run the pipeline job\n",
    "# job.run(sync=False)\n",
    "\n",
    "job.submit(\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    service_account=SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab987b5-f907-4aea-a80f-374d21b5f50a",
   "metadata": {},
   "source": [
    "### inspect config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cf385f49-644f-4acd-b1c5-21191d0a15fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bucket_uri: gs://tpu-pipeline-hybrid-vertex\n",
       "deploy_compute: n1-standard-4\n",
       "deploy_gpu: NVIDIA_TESLA_T4\n",
       "deploy_gpu_count: 1\n",
       "deploy_image: us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-gpu.2-13:latest\n",
       "endpoint_id: '4267284891747483648'\n",
       "epochs: 15\n",
       "location: us-central1\n",
       "model_display_name: cifar10-class-model\n",
       "pipeline_local_json: tpu_train_cifar10_pipeline.json\n",
       "pipeline_name: train-endpoint-deploy-v10\n",
       "pipeline_root: gs://tpu-pipeline-hybrid-vertex/pipeline_root/tpu_cifar10_pipeline\n",
       "project_id: hybrid-vertex\n",
       "repository: my-tpu-repo\n",
       "seed: null\n",
       "service_account: 934903580331-compute@developer.gserviceaccount.com\n",
       "steps: 10000\n",
       "tb_instance: projects/934903580331/locations/us-central1/tensorboards/257140585364717568\n",
       "train_compute: cloud-tpu\n",
       "train_image: us-central1-docker.pkg.dev/hybrid-vertex/my-tpu-repo/tpu-train:latest\n",
       "train_strategy: tpu\n",
       "train_tpu: TPU_V2\n",
       "train_tpu_count: 8\n",
       "trainer_args:\n",
       "- --epochs=15\n",
       "- --steps=10000\n",
       "- --distribute=tpu\n",
       "- --project=hybrid-vertex\n",
       "- --tb_instance=projects/934903580331/locations/us-central1/tensorboards/257140585364717568\n",
       "- --experiment_name=tpu-train-v10\n",
       "- --experiment_run=run-b1024\n",
       "- --batch_size=1024\n",
       "working_dir: null"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34854281-3853-45ac-b294-a8c3a38c7336",
   "metadata": {},
   "source": [
    "## Get pipeline task details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1320108-b59b-4640-85a8-c512d31c8d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_detail(\n",
    "    task_details: List[Dict[str, Any]], task_name: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Function to fetch the details from the specifed task name\"\"\"\n",
    "    for task_detail in task_details:\n",
    "        if task_detail.task_name == task_name:\n",
    "            return task_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134aa82-a80d-46e4-a0b3-12eda2c86518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pipeline task details\n",
    "pipeline_task_details = (\n",
    "    job.gca_resource.job_detail.task_details\n",
    ")  # fetch pipeline task details\n",
    "\n",
    "pipeline_task_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a2fc5-2eb6-4991-b52a-3b6abb5e8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetch endpoint from pipeline\n",
    "# endpoint_task = get_task_detail(pipeline_task_details, \"endpoint-create\")\n",
    "# endpoint_resourceName = (\n",
    "#     endpoint_task.outputs[\"endpoint\"].artifacts[0].metadata[\"resourceName\"]\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(endpoint_resourceName)\n",
    "# endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d759fa0-900f-4da6-8d76-51bec932d67a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Fetch model from pipeline\n",
    "# model_task = get_task_detail(pipeline_task_details, \"model-upload\")\n",
    "# model_resourceName = model_task.outputs[\"model\"].artifacts[0].metadata[\"resourceName\"]\n",
    "# model = aiplatform.Model(model_resourceName)\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c082978-2b18-4dd3-aee8-e31dd43c5e7c",
   "metadata": {},
   "source": [
    "### delete if necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b184c46-fda7-4b30-b45d-61e195c8909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Undeploy model from endpoint\n",
    "# endpoint.undeploy_all()\n",
    "\n",
    "# # Delete the endpoint\n",
    "# endpoint.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c267bdd-e6e4-4a1a-8fdb-6e4af507b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete the model\n",
    "# model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f677cba-a841-42b4-8019-773383392a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete the pipeline job\n",
    "# job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409d1b8-6712-48c9-9fcc-16f7be02578d",
   "metadata": {},
   "source": [
    "**Finished**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f48b6c-07dc-4bf9-83e1-fbf1f8e3ad3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ac6c5-a10d-442e-a6d5-fa3e867e1e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1604c3-ed9b-4ace-af30-63ede097a853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b0090-f0b4-45f0-a8e0-d518514b1816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d19e7c-e482-4fbd-bad3-a70ce462906b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3bd3f2-1562-4341-a2b6-1722c3f43cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765047f4-464b-4acf-949d-56b6db5bbd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a600f4-d391-4775-a46a-ecd01638827e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edea73e-a33d-4655-a691-b9b597a6810f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053ed6b-476a-485d-b164-9c9a0168230e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8cac4-6f99-4a33-b943-ef8dd21cc979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb74350-3526-4450-b0b6-b0a883c0a7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e8fae-7184-452e-8bbc-d9c639044ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385777a1-70cb-409b-b0f7-e23cfcafdfc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
